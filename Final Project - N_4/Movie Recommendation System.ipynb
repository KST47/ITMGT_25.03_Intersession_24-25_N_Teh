{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e28e3fd-b3e4-4efa-9805-cdc7676f22f0",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ebcd66-5162-4654-869d-333f30d24513",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import process\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e14af9-e504-48fb-8de6-aca8bdccdaac",
   "metadata": {},
   "source": [
    "## Note:\n",
    "\n",
    "There were two datasets used in this project (the links are provided at the end of this cell).\n",
    "Initially, two DataFrames were created using each dataset, then merged to create merged_df.\n",
    "However, one of the datasets (485 MB) is too large to be pushed to GitHub. \n",
    "As a result, we instead saved the merged DataFrame to a new CSV in the directory named TMDB_merged_df.csv.\n",
    "The merged_df seen in this program is loaded from that file.\n",
    "The original code for loading and merging the two datasets is commented in the next three cells.\n",
    "\n",
    "Links to the datasets used in the project:\n",
    "\n",
    "https://www.kaggle.com/datasets/sankha1998/tmdb-top-10000-popular-movies-dataset (TMDb_updated.csv)\n",
    "\n",
    "https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies (TMDB_movie_dataset_v11.csv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab5e833-0810-4db1-a95f-710f796b60d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_1 = pd.read_csv(\"TMDb_updated.csv\",index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e2997-fcf4-4433-bcfe-f9a8a5c04e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2 = pd.read_csv(\"TMDB_movie_dataset_v11.csv\")\n",
    "# df_2[\"overview\"] = df_2[\"overview\"].fillna(\"\")\n",
    "# df_2 = df_2.drop_duplicates(subset=[\"title\", \"release_date\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39466702-f361-43b3-9699-2e3854c8b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df = df_1.merge(df_2[[\"title\", \"release_date\", \"overview\",\"genres\"]], on = [\"title\",\"overview\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2f9b4-5257-4ee8-a127-da3edb392fd1",
   "metadata": {},
   "source": [
    "# Loading the Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bfd58d-4fe3-45b5-94f9-b6d2f4e7a351",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(\"TMDB_merged_df.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db6673f-78f8-4340-882f-bb7f08387d4c",
   "metadata": {},
   "source": [
    "# Filtering Movies\n",
    "In this dataset, there are some movies that have a vote_average but have no vote_counts, which doesn't make sense. There are also duplicates of the same movie. So we filtered it to get more accurate data. \n",
    "\n",
    "We only included the important data only (id, title, vote_average, vote_count, overview, genres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2d8a7-415c-497b-a2b6-8a2a1b412207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_df[~(merged_df[\"vote_count\"] == 0)] # Removes movies with no available data for vote count\n",
    "filtered_df = filtered_df[[\"title\", \"release_date\", \"overview\", \"genres\", \"vote_average\", \"vote_count\"]] # Filters only the details needed\n",
    "filtered_df = filtered_df.drop_duplicates(subset=[\"title\", \"overview\"], keep=\"first\") # Removes duplicates\n",
    "filtered_df[\"overview\"] = filtered_df[\"overview\"].fillna(\"\") #For overviews with NaN values\n",
    "filtered_df[\"genres\"] = filtered_df[\"genres\"].fillna(\"\") # For genres with NaN values\n",
    "filtered_df = filtered_df[~(filtered_df[\"genres\"] == \"\")]\n",
    "filtered_df[\"genres\"] = filtered_df[\"genres\"].apply(lambda x: x.split(', '))  # Separating the genres\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "filtered_df = filtered_df.reset_index()\n",
    "filtered_df = filtered_df.drop(\"index\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0fc6e-100d-4b94-827b-49d0e1db29dc",
   "metadata": {},
   "source": [
    "# Content-Based Filtering\n",
    "\n",
    "## sklearn.preprocessing\n",
    "The sklearn.preprocessing package provides several common utility functions and transformer classes to change raw feature vectors into a representation that is more suitable for the downstream estimators. \n",
    "\n",
    "https://scikit-learn.org/0.19/modules/preprocessing.html#:~:text=The%20sklearn.,standardization%20of%20the%20data%20set.\n",
    "\n",
    "#### MultiLabelBinarizer\n",
    "MultiLabelBinarizer transforms between iterable of iterables and a multilabel format. \r\n",
    "Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class labe\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7113c-fd1f-4694-8043-4109b7dd4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding genres\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded = mlb.fit_transform(filtered_df['genres'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6bcda-ed11-4baa-b129-ebc023af2d07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = filtered_df.copy()\n",
    "test_df[\"genre_vector\"] = genres_encoded.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a017d4a-14c8-4617-8dec-b70a93e2cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vote_count = test_df[\"vote_count\"].mean()\n",
    "mean_vote_average = test_df[\"vote_average\"].mean()\n",
    "def bayesian_avg(vote_average, vote_count):\n",
    "    bayesian_average = (mean_vote_count * mean_vote_average + vote_average * vote_count) / (mean_vote_count + vote_count)\n",
    "    return round(bayesian_average, 1)\n",
    "test_df[\"bayesian_avg\"] = bayesian_avg(test_df[\"vote_average\"], test_df[\"vote_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9ab3dc-5c02-485c-8d5a-46088bac8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_similarity(input_bayesian_avg, test_bayesian_avg):\n",
    "    euclidean_distance = abs(input_bayesian_avg - test_bayesian_avg)\n",
    "    return 1 / (1 + euclidean_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfec273-4b5d-42ae-8d07-a9ce94ecd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def recommendations(input_index):\n",
    "    recommendations_df = test_df.copy()\n",
    "    input_vector = np.array(recommendations_df.loc[input_index, \"genre_vector\"])\n",
    "    genre_vectors_array = np.array(recommendations_df[\"genre_vector\"].tolist())\n",
    "    cosine_similarities = cosine_similarity([input_vector], genre_vectors_array)\n",
    "    recommendations_df[\"genre_similarity\"] = cosine_similarities[0].tolist()\n",
    "    \n",
    "    input_bayesian_avg = recommendations_df.loc[input_index, \"bayesian_avg\"]\n",
    "    recommendations_df[\"rating_similarity\"] = rating_similarity(input_bayesian_avg, recommendations_df[\"bayesian_avg\"])\n",
    "\n",
    "    alpha = 0.65\n",
    "    recommendations_df[\"overall_similarity\"] = alpha * recommendations_df[\"genre_similarity\"] + (1 - alpha) * recommendations_df[\"rating_similarity\"]\n",
    "    recommendations_df = recommendations_df.drop(index = input_index)\n",
    "    recommendations_df = recommendations_df.sort_values(by = \"overall_similarity\", ascending = False)\n",
    "\n",
    "    return recommendations_df[[\"title\", \"overview\",\"genres\",\"vote_average\",\"vote_count\"]][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d8c39-f35a-49e2-921d-701f9ce77a42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_list = test_df[\"title\"].tolist()\n",
    "found_movie_input = \"\"\n",
    "\n",
    "while found_movie_input == \"\":\n",
    "    input_movie = input(\"Hello! Please enter a movie you would like to receive recommendations for.\")\n",
    "    possible_matches_tuples = process.extractBests(input_movie, movie_list, score_cutoff = 80, limit = 10)\n",
    "    possible_matches_list = [x[0] for x in possible_matches_tuples]\n",
    "    if len(possible_matches_list) == 0:\n",
    "        continue_input = input(\"It seems that movie does not exist in the database. Would you like to try again? Type N for No, and anything else for Yes.\").strip()\n",
    "        if \"N\" == continue_input.upper():\n",
    "            print(\"Thank you, have a nice day!\")\n",
    "            break\n",
    "        else:\n",
    "            clear_output()\n",
    "            print(\"Understood. Let's try again!\")\n",
    "            continue\n",
    "    else:\n",
    "        print(\"These were the top matches for your query.\")\n",
    "        possible_matches_df = test_df[test_df[\"title\"].isin(possible_matches_list)]\n",
    "        display(possible_matches_df[[\"title\",\"release_date\",\"overview\",\"genres\"]])\n",
    "        found_movie_input = input(\"Does this show the movie you want? If so, type Y for Yes, and anything else for No.\").strip()\n",
    "        if \"Y\" != found_movie_input.upper():\n",
    "            continue_input = input(\"Would you like to try another search? Type N for No, and anything else for Yes.\").strip()\n",
    "            if \"N\" == continue_input.upper():\n",
    "                print(\"Thank you, have a nice day!\")\n",
    "                break\n",
    "            else:\n",
    "                clear_output()\n",
    "                found_movie_input = \"\"\n",
    "                print(\"Understood. Let's try again!\")\n",
    "else: \n",
    "    while True:\n",
    "        input_index = input(\"Type the number on the left of the title to proceed.\").strip()\n",
    "        valid_index_list = [str(x) for x in possible_matches_df.index]\n",
    "        if input_index in valid_index_list:\n",
    "            clear_output()\n",
    "            print(\"Thank you! Below are 10 recommendations for that movie:\")\n",
    "            display(recommendations(int(input_index)))\n",
    "            break\n",
    "        else: \n",
    "            print(\"It seems you inputted the wrong index. Please try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
